{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0e279f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/parallelization.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239934-lesson-1-parallelization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4169bfb-769a-4db3-833e-c827f19024b2",
   "metadata": {},
   "source": [
    "# Parallel node execution\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 3, we went in-depth on `human-in-the loop`, showing 3 common use-cases:\n",
    "\n",
    "(1) `Approval` - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
    "\n",
    "(2) `Debugging` - We can rewind the graph to reproduce or avoid issues\n",
    "\n",
    "(3) `Editing` - You can modify the state \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will build on `human-in-the-loop` as well as the `memory` concepts discussed in module 2.\n",
    "\n",
    "We will dive into `multi-agent` workflows, and build up to a multi-agent research assistant that ties together all of the modules from this course.\n",
    "\n",
    "To build this multi-agent research assistant, we'll first discuss a few LangGraph controllability topics.\n",
    "\n",
    "We'll start with [parallelization](https://langchain-ai.github.io/langgraph/how-tos/branching/#how-to-create-branches-for-parallel-node-execution).\n",
    "\n",
    "## Fan out and fan in\n",
    "\n",
    "Let's build a simple linear graph that over-writes the state at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618eab5c-4ef7-4273-8e0b-a9c847897ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U  langgraph tavily-python wikipedia langchain_openai langchain_community langgraph_sdk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd77093-1794-4bd7-8c57-58f59a74c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from typing import Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class State(TypedDict):\n",
    "    # The operator.add reducer fn makes this append-only\n",
    "    state: str\n",
    "\n",
    "class ReturnNodeValue:\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['state']}\")\n",
    "        return {\"state\": [self._value]}\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"b\", \"c\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd027d3-ef9f-4d43-b190-e9f07d521e18",
   "metadata": {},
   "source": [
    "We over-write state, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf260088-90d5-45b2-93ab-42f241560840",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"state\": []})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dec27d-dc43-4088-beb2-53ad090d2971",
   "metadata": {},
   "source": [
    "Now, let's run `b` and `c` in parallel. \n",
    "\n",
    "And then run `d`.\n",
    "\n",
    "We can do this easily with fan-out from `a` to `b` and `c`, and then fan-in to `d`.\n",
    "\n",
    "The the state updates are applied at the end of each step.\n",
    "\n",
    "Let's run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeaaab-a8c3-470f-89ef-9cf0a2760667",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35238fde-0230-4ae8-9200-158a8835c4f1",
   "metadata": {},
   "source": [
    "**We see an error**! \n",
    "\n",
    "This is because both `b` and `c` are writing to the same state key / channel in the same step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048b041-6849-4f09-9811-6b7a80f67859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import InvalidUpdateError\n",
    "try:\n",
    "    graph.invoke({\"state\": []})\n",
    "except InvalidUpdateError as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc329d-59fa-4c26-adcf-9122a824955d",
   "metadata": {},
   "source": [
    "When using fan out, we need to be sure that we are using a reducer if steps are writing to the same the channel / key. \n",
    "\n",
    "As we touched on in Module 2, `operator.add` is a function from Python's built-in operator module.\n",
    "\n",
    "When `operator.add` is applied to lists, it performs list concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1292ac-510a-4801-b2a3-e2c6d2d9582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "\n",
    "class State(TypedDict):\n",
    "    # The operator.add reducer fn makes this append-only\n",
    "    state: Annotated[list, operator.add]\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbad231-fc1d-49b1-a9fc-ed9153fa3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"state\": []})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5baa2-cecd-44b6-b0c4-d258340783f8",
   "metadata": {},
   "source": [
    "Now we see that we append to state for the updates made in parallel by `b` and `c`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fc7c7-198d-41be-867f-e77c93ba3217",
   "metadata": {},
   "source": [
    "## Waiting for nodes to finish\n",
    "\n",
    "Now, lets consider a case where one parallel path has more steps than the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b5d4f-dd39-4c22-b623-e0abc23f9144",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"b2\")\n",
    "builder.add_edge([\"b2\", \"c\"], \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11640e6f-ac62-4ad4-89d9-7f6f9b56bf7a",
   "metadata": {},
   "source": [
    "In this case, `b`, `b2`, and `c` are all part of the same step.\n",
    "\n",
    "The graph will wait for all of these to be completed before proceeding to step `d`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafda930-e75b-410f-ba93-eb5fc0219303",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"state\": []})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610a2e3-b053-47e8-bf4e-0968dfaa0a5d",
   "metadata": {},
   "source": [
    "## Setting the order of state updates\n",
    "\n",
    "However, within each step we don't have specific control over the order of the state updates!\n",
    "\n",
    "In simple terms, it is a deterministic order determined by LangGraph based upon graph topology that **we do not control**. \n",
    "\n",
    "Above, we see that `c` is added before `b2`.\n",
    "\n",
    "However, we can use a custom reducer to customize this e.g., sort state updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24788e73-0950-432e-ad32-7987ea076529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_reducer(left, right):\n",
    "    \"\"\" Combines and sorts the values in a list\"\"\"\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "\n",
    "    return sorted(left + right, reverse=False)\n",
    "\n",
    "class State(TypedDict):\n",
    "    # sorting_reducer will sort the values in state\n",
    "    state: Annotated[list, sorting_reducer]\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"b2\")\n",
    "builder.add_edge([\"b2\", \"c\"], \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607dba2e-f9f0-4bc7-8ba6-684521a49bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"state\": []})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1714c0-e881-48e7-bcb8-a60016f0485e",
   "metadata": {},
   "source": [
    "Now, the reducer sorts the updated state values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0750b-e6af-40d9-835c-c664da5a2d3b",
   "metadata": {},
   "source": [
    "## Working with LLMs\n",
    "\n",
    "Now, lets add a realistic example! \n",
    "\n",
    "We want to gather context from two external sources (Wikipedia and Web-Seach) and have an LLM answer a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9d03c-cb41-415c-862d-c9616d5a2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75cc78-d1a1-47a5-8648-bf5a79c883de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    context: Annotated[list, operator.add]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e714ea8-095c-461a-98bc-ee782a84ef5c",
   "metadata": {},
   "source": [
    "You can try different web search tools. [Tavily](https://tavily.com/) is one nice option to consider, but ensure your `TAVILY_API_KEY` is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb519a-d08a-4ec7-8f0b-2ce6a9bf7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "_set_env(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4f56c-3334-4927-8ed8-62fd384ee43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "def search_web(state):\n",
    "\n",
    "    \"\"\" Retrieve docs from web search \"\"\"\n",
    "\n",
    "    # Search\n",
    "    tavily_search = TavilySearchResults(max_results=3)\n",
    "    search_docs = tavily_search.invoke(state['question'])\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "def search_wikipedia(state):\n",
    "\n",
    "    \"\"\" Retrieve docs from wikipedia \"\"\"\n",
    "\n",
    "    # Search\n",
    "    search_docs = WikipediaLoader(query=state['question'],\n",
    "                                  load_max_docs=2).load()\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "def generate_answer(state):\n",
    "\n",
    "    \"\"\" Node to answer a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    context = state[\"context\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Template\n",
    "    answer_template = \"\"\"Answer the question {question} using this context: {context}\"\"\"\n",
    "    answer_instructions = answer_template.format(question=question,\n",
    "                                                       context=context)\n",
    "\n",
    "    # Answer\n",
    "    answer = llm.invoke([SystemMessage(content=answer_instructions)]+[HumanMessage(content=f\"Answer the question.\")])\n",
    "\n",
    "    # Append it to state\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret\n",
    "builder.add_node(\"search_web\",search_web)\n",
    "builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
    "builder.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"search_wikipedia\")\n",
    "builder.add_edge(START, \"search_web\")\n",
    "builder.add_edge(\"search_wikipedia\", \"generate_answer\")\n",
    "builder.add_edge(\"search_web\", \"generate_answer\")\n",
    "builder.add_edge(\"generate_answer\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa544ca0-10af-491e-ad7a-477d004413eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\"question\": \"How were Cisco's Q2 2024 earnings\"})\n",
    "result['answer'].content\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dbbecab-80eb-4f0c-b43a-45542fc0ae9c",
   "metadata": {},
   "source": [
    "## Using with LangGraph API\n",
    "\n",
    "--\n",
    "\n",
    "**⚠️ DISCLAIMER**\n",
    "\n",
    "*Running Studio currently requires a Mac. If you are not using a Mac, then skip this step.*\n",
    "\n",
    "*Also, if you are running this notebook in CoLab, then skip this step.*\n",
    "\n",
    "--\n",
    "\n",
    "Let's load our the above graph in the Studio UI, which uses `module-4/studio/parallelization.py` set in `module-4/studio/langgraph.json`.\n",
    "\n",
    "![Screenshot 2024-08-29 at 3.05.13 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb10f43c3d4df239e0278_parallelization-1.png)\n",
    "\n",
    "Let's get the URL for the local deployment from Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc8ad8d-1365-4801-a8a5-b85cd4965119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "if 'google.colab' in str(get_ipython()) or platform.system() != 'Darwin':\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab or requires a Mac\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23919dc9-27d8-4d10-b91d-24acdf8c0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "# client = get_client(url=\"http://localhost:63082\")\n",
    "client = get_client(url=\"http://localhost:63338\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35e68f-4017-4f45-93cf-ddbb355a0bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = await client.threads.create()\n",
    "input_question = {\"question\": \"How were Nvidia Q2 2024 earnings?\"}\n",
    "async for event in client.runs.stream(thread[\"thread_id\"],\n",
    "                                      assistant_id=\"parallelization\",\n",
    "                                      input=input_question,\n",
    "                                      stream_mode=\"values\"):\n",
    "    # Check if answer has been added to state\n",
    "    answer = event.data.get('answer', None)\n",
    "    if answer:\n",
    "        print(answer['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
